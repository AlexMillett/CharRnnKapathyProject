{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x5kdu9WO7aAp"
      },
      "source": [
        "<a\n",
        "href=\"https://colab.research.google.com/github/wingated/cs474_labs_f2019/blob/master/DL_Lab6.ipynb\"\n",
        "  target=\"_parent\">\n",
        "  <img\n",
        "    src=\"https://colab.research.google.com/assets/colab-badge.svg\"\n",
        "    alt=\"Open In Colab\"/>\n",
        "</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cksgAH12XRjV"
      },
      "source": [
        "# Sequence-to-sequence models with karpathy\n",
        "\n",
        "### Description:\n",
        "For this project, I will code up the [char-rnn model of Karpathy](http://karpathy.github.io/2015/05/21/rnn-effectiveness/). This is a recurrent neural network that is trained probabilistically on sequences of characters, and that can then be used to sample new sequences that are like the original.\n",
        "\n",
        "i\n",
        "### Example Output:\n",
        "An example of karpathy final samples are shown below (more detail in the\n",
        "final section of this writeup), after 150 passes through the data.\n",
        "Please generate about 15 samples for each dataset.\n",
        "\n",
        "<code>\n",
        "And ifte thin forgision forward thene over up to a fear not your\n",
        "And freitions, which is great God. Behold these are the loss sub\n",
        "And ache with the Lord hath bloes, which was done to the holy Gr\n",
        "And appeicis arm vinimonahites strong in name, to doth piseling\n",
        "And miniquithers these words, he commanded order not; neither sa\n",
        "And min for many would happine even to the earth, to said unto m\n",
        "And mie first be traditions? Behold, you, because it was a sound\n",
        "And from tike ended the Lamanites had administered, and I say bi\n",
        "</code>\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LunlKY9bQbiO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! wget -O ./text_files.tar.gz 'https://piazza.com/redirect/s3?bucket=uploads&prefix=attach%2Fjlifkda6h0x5bk%2Fhzosotq4zil49m%2Fjn13x09arfeb%2Ftext_files.tar.gz'\n",
        "! tar -xzf text_files.tar.gz\n",
        "! pip install unidecode\n",
        "!pip install torch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jFtJY06J8H1R",
        "outputId": "0d83704d-6920-41c5-9237-8610d13a93f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-02-17 20:23:01--  https://piazza.com/redirect/s3?bucket=uploads&prefix=attach%2Fjlifkda6h0x5bk%2Fhzosotq4zil49m%2Fjn13x09arfeb%2Ftext_files.tar.gz\n",
            "Resolving piazza.com (piazza.com)... 3.215.61.73, 35.174.203.227, 23.22.156.213, ...\n",
            "Connecting to piazza.com (piazza.com)|3.215.61.73|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cdn-uploads.piazza.com/attach/jlifkda6h0x5bk/hzosotq4zil49m/jn13x09arfeb/text_files.tar.gz [following]\n",
            "--2024-02-17 20:23:01--  https://cdn-uploads.piazza.com/attach/jlifkda6h0x5bk/hzosotq4zil49m/jn13x09arfeb/text_files.tar.gz\n",
            "Resolving cdn-uploads.piazza.com (cdn-uploads.piazza.com)... 18.164.154.125, 18.164.154.113, 18.164.154.110, ...\n",
            "Connecting to cdn-uploads.piazza.com (cdn-uploads.piazza.com)|18.164.154.125|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1533290 (1.5M) [application/x-gzip]\n",
            "Saving to: ‘./text_files.tar.gz’\n",
            "\n",
            "./text_files.tar.gz 100%[===================>]   1.46M  --.-KB/s    in 0.09s   \n",
            "\n",
            "2024-02-17 20:23:01 (17.1 MB/s) - ‘./text_files.tar.gz’ saved [1533290/1533290]\n",
            "\n",
            "Requirement already satisfied: unidecode in /usr/local/lib/python3.10/dist-packages (1.3.8)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.1.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.9.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l7bdZWxvJrsx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c7a9a5e-7287-4932-8aba-0b355530b63f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "file_len = 2579888\n"
          ]
        }
      ],
      "source": [
        "import unidecode\n",
        "import string\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "all_characters = string.printable\n",
        "n_characters = len(all_characters)\n",
        "file = unidecode.unidecode(open('./text_files/lotr.txt').read())\n",
        "file_len = len(file)\n",
        "print('file_len =', file_len)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TxBeKeNjJ0NQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c04a6d14-26ff-4e3f-d579-0a41713edaff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "id Frodo. 'If we make as good going this afternoon as we \n",
            "have done this morning, we shall have left the Downs before the Sun sets and \n",
            "be jogging on in search of a camping place.' But even as he spoke\n"
          ]
        }
      ],
      "source": [
        "chunk_len = 200\n",
        "\n",
        "def random_chunk():\n",
        "  start_index = random.randint(0, file_len - chunk_len)\n",
        "  end_index = start_index + chunk_len + 1\n",
        "  return file[start_index:end_index]\n",
        "\n",
        "print(random_chunk())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "On0_WitWJ99e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8319f68-2df6-4276-9892-045696d3f048"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([10, 11, 12, 39, 40, 41])\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Turn string into list of longs\n",
        "def char_tensor(string):\n",
        "  tensor = torch.zeros(len(string)).long()\n",
        "  for c in range(len(string)):\n",
        "      tensor[c] = all_characters.index(string[c])\n",
        "  return tensor\n",
        "\n",
        "print(char_tensor('abcDEF'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aavAv50ZKQ-F"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class GRU(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size, num_layers):\n",
        "    super(GRU, self).__init__()\n",
        "\n",
        "    self.input_size = input_size\n",
        "    self.hidden_size = hidden_size\n",
        "    self.num_layers = num_layers\n",
        "\n",
        "    self.sigmoid = nn.Sigmoid()\n",
        "    self.tanh = nn.Tanh()\n",
        "\n",
        "    self.lin_ir = nn.Linear(self.input_size, self.hidden_size, bias=True)\n",
        "    self.lin_hr = nn.Linear(self.hidden_size, self.hidden_size, bias=True)\n",
        "    self.lin_iz = nn.Linear(self.input_size, self.hidden_size, bias=True)\n",
        "    self.lin_hz = nn.Linear(self.hidden_size, self.hidden_size, bias=True)\n",
        "    self.lin_in = nn.Linear(self.input_size, self.hidden_size, bias=True)\n",
        "    self.lin_hn = nn.Linear(self.hidden_size, self.hidden_size, bias=True)\n",
        "\n",
        "    self.b_ir = nn.Parameter(torch.zeros(hidden_size))\n",
        "    self.b_hr = nn.Parameter(torch.zeros(hidden_size))\n",
        "    self.b_iz = nn.Parameter(torch.zeros(hidden_size))\n",
        "    self.b_hz = nn.Parameter(torch.zeros(hidden_size))\n",
        "    self.b_in = nn.Parameter(torch.zeros(hidden_size))\n",
        "    self.b_hn = nn.Parameter(torch.zeros(hidden_size))\n",
        "\n",
        "  def forward(self, inputs, hidden):\n",
        "    # Each layer does the following:\n",
        "    # r_t = sigmoid(W_ir*x_t + b_ir + W_hr*h_(t-1) + b_hr)\n",
        "\n",
        "    r_t = self.sigmoid(self.lin_ir(inputs) + self.b_ir + self.lin_hr(hidden) + self.b_hr)\n",
        "    # z_t = sigmoid(W_iz*x_t + b_iz + W_hz*h_(t-1) + b_hz)\n",
        "    z_t = self.sigmoid(self.lin_iz(inputs) + self.b_iz + self.lin_hz(hidden) + self.b_hz)\n",
        "    # n_t = tanh(W_in*x_t + b_in + r_t**(W_hn*h_(t-1) + b_hn))\n",
        "    n_t = self.tanh(self.lin_in(inputs) + self.b_in + (r_t**(self.lin_hn(hidden) + self.b_hn)))\n",
        "    # h_(t) = (1 - z_t)**n_t + z_t**h_(t-1)\n",
        "    hiddens = ((1 - z_t) * n_t) + (z_t * hidden)\n",
        "    outputs = hiddens[-1:]\n",
        "    # Your code here\n",
        "    # Where ** is hadamard product (not matrix multiplication, but elementwise multiplication)\n",
        "\n",
        "    return outputs, hiddens\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d6tNdEnzWj5F"
      },
      "outputs": [],
      "source": [
        "\n",
        "class Decoder_RNN(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size, output_size, n_layers=1):\n",
        "    super(Decoder_RNN, self).__init__()\n",
        "    self.input_size = input_size\n",
        "    self.hidden_size = hidden_size\n",
        "    self.output_size = output_size\n",
        "    self.n_layers = n_layers\n",
        "    self.embedding = nn.Embedding(input_size, hidden_size)\n",
        "    self.gru = GRU(hidden_size, hidden_size, num_layers=self.n_layers)\n",
        "    self.out = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "\n",
        "  def forward(self, input_char, hidden):\n",
        "        output = self.embedding(input_char).view(1,1,-1)\n",
        "        output = F.relu(output)\n",
        "        # output = output.unsqueeze(0)\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "        output = self.out(output)\n",
        "        return output, hidden\n",
        "\n",
        "  def init_hidden(self):\n",
        "    return torch.zeros(self.n_layers, 1, self.hidden_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hrhXghEPKD-5"
      },
      "outputs": [],
      "source": [
        "def random_training_set():\n",
        "  chunk = random_chunk()\n",
        "  inp = char_tensor(chunk[:-1])\n",
        "  target = char_tensor(chunk[1:])\n",
        "  return inp, target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ALC3Pf8Kbsi"
      },
      "outputs": [],
      "source": [
        "# NOTE: decoder_optimizer, decoder, and criterion will be defined below as global variables\n",
        "def train(inp, target):\n",
        "  ## initialize hidden layers, set up gradient and loss\n",
        "    # your code here\n",
        "  ## /\n",
        "  #  for char_input, char_tar in zip(inp, target):\n",
        "\n",
        "  decoder_optimizer.zero_grad()\n",
        "  hidden = decoder.init_hidden()\n",
        "  loss = 0\n",
        "  objective = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "  for char_input,char_tar in zip(inp,target):\n",
        "    char_dec,hidden = decoder.forward(char_input,hidden)\n",
        "    char_hat = char_dec.squeeze(0)\n",
        "    char_truth = char_tar.unsqueeze(0)\n",
        "\n",
        "    loss += objective(char_hat, char_truth)\n",
        "\n",
        "  loss.backward()\n",
        "  decoder_optimizer.step()\n",
        "  return loss.item() / len(inp)\n",
        "\n",
        "\n",
        "  # more stuff here..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B-bp-OZ1KjNh"
      },
      "outputs": [],
      "source": [
        "def sample_outputs(output, temperature):\n",
        "    \"\"\"Takes in a vector of unnormalized probability weights and samples a character from the distribution\"\"\"\n",
        "    # As temperature approaches 0, this sampling function becomes argmax (no randomness)\n",
        "    # As temperature approaches infinity, this sampling function becomes a purely random choice\n",
        "    return torch.multinomial(torch.exp(output / temperature), 1)\n",
        "\n",
        "def evaluate(prime_str='A', predict_len=100, temperature=0.8):\n",
        "  ## initialize hidden state, initialize other useful variables\n",
        "    # your code here\n",
        "  ## /\n",
        "  hidden = decoder.init_hidden()\n",
        "  prime_tens = char_tensor(prime_str[:-1])\n",
        "  print(prime_str)\n",
        "  total_string = prime_str\n",
        "  curr_tens = prime_tens[-1]\n",
        "\n",
        "  for i in range(len(prime_tens)-1):\n",
        "    curr_out = decoder.forward(prime_tens[i],hidden)\n",
        "    hidden = curr_out[1]\n",
        "\n",
        "  i = 0\n",
        "  while i < predict_len:\n",
        "\n",
        "    curr_out,hidden = decoder.forward(curr_tens,hidden)\n",
        "\n",
        "    y_hat = curr_out\n",
        "    y_hat = y_hat.squeeze(0)\n",
        "    y_hat = y_hat.squeeze(0)\n",
        "\n",
        "\n",
        "    i += 1\n",
        "    curr_tens = sample_outputs(y_hat, temperature).squeeze(0)\n",
        "    total_string += all_characters[curr_tens.item()]\n",
        "\n",
        "  return total_string\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-nXFeCmdKodw"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "n_epochs = 1500\n",
        "print_every = 200\n",
        "plot_every = 10\n",
        "hidden_size = 200\n",
        "n_layers = 3\n",
        "lr = 0.001\n",
        "\n",
        "decoder = Decoder_RNN(n_characters, hidden_size, n_characters, n_layers)\n",
        "decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=lr)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "start = time.time()\n",
        "all_losses = []\n",
        "loss_avg = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xKfozqw-6eqb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e86db5d-3194-4a2d-ba45-958be91e2136"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[78.37136936187744 (200 13%) 2.2466]\n",
            "Wh\n",
            "Whon thed avo sing wat \n",
            "he were an soen the thon the a \n",
            "staid nor, 'ord, not his. \n",
            "\n",
            "bo Emway nour loon \n",
            "\n",
            "[154.08296608924866 (400 26%) 2.0693]\n",
            "Wh\n",
            "Whtce and and \n",
            "and and the wirken ther the lfime.' \n",
            "\n",
            "\n",
            "\n",
            "Theare as the Fill onterled ilf. They 'ind a th \n",
            "\n",
            "[229.0678050518036 (600 40%) 2.0405]\n",
            "Wh\n",
            "Whast of this for the seling to Gimut the \n",
            "break muss a suim at shour not you couth might wat mouthe,  \n",
            "\n",
            "[303.79496145248413 (800 53%) 2.1669]\n",
            "Wh\n",
            "Whe warse he beend as a gusten the stien but of reen to be the gone the maste. To rneyous the mught it \n",
            "\n",
            "[378.5814447402954 (1000 66%) 1.7240]\n",
            "Wh\n",
            "Whare to of to worred, in at have had the \n",
            "\n",
            "mind of the pay sin go light of my tamsed. GBeotely put to \n",
            "\n",
            "[453.36096382141113 (1200 80%) 1.6382]\n",
            "Wh\n",
            "Whe the had by was Geetwer a as athalf in the asting lan awas past assen sace \n",
            "hes, as and sterered tr \n",
            "\n",
            "[528.2648463249207 (1400 93%) 1.7955]\n",
            "Wh\n",
            "Whher it crise of mort-' hearders ever stame not this leaters. He said Sare turned \n",
            "they said the Righ \n",
            "\n"
          ]
        }
      ],
      "source": [
        "# n_epochs = 2000\n",
        "for epoch in range(1, n_epochs + 1):\n",
        "  loss_ = train(*random_training_set())\n",
        "  loss_avg += loss_\n",
        "\n",
        "  if epoch % print_every == 0:\n",
        "      print('[%s (%d %d%%) %.4f]' % (time.time() - start, epoch, epoch / n_epochs * 100, loss_))\n",
        "      print(evaluate('Wh', 100), '\\n')\n",
        "\n",
        "  if epoch % plot_every == 0:\n",
        "      all_losses.append(loss_avg / plot_every)\n",
        "      loss_avg = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ee0so6aKJ5L8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d64b1a5-40ba-4277-8ff5-ec88ba4c2b6d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " lo\n",
            " lo\n",
            " loand sloke in bots of the yan dould slone. Ato reampan to noges, start of the \n",
            "cat in you hous and the nown leastrods the took-to go like \n",
            "have to many. 'Ovey comes can tonge: I't a lippors in as it re \n",
            "\n",
            " G\n",
            " G\n",
            " Gsaid Fres of is now nou shice the have? \n",
            "\n",
            "Sould has dis-to and the ching, just and no starring even saids were the \n",
            "\n",
            "\n",
            "Hentiel pindless \n",
            "and it sunga of trees lowch the \n",
            "of the Shoughs in the away. But \n",
            "\n",
            " ca\n",
            " ca\n",
            " caound riston in. Bome the borked ares onder cain grims shaw gasting of the all in \n",
            "the with \n",
            "And like cove that of he had great more to \n",
            "Ring stowning of Elves in the birth, and the siepter not of the  \n",
            "\n",
            " Th\n",
            " Th\n",
            " Thhat hil and was \n",
            "the ridk the parged that the Blearry prean-the enceed in dark was bed them stome of The that now is in the had said Frodow. Gome them the foring of the Nolling of finght of the Ladbre \n",
            "\n",
            " ra\n",
            " ra\n",
            " raeeps, fell, to have \n",
            "as in the King recho sooding the mong told it he had fount befulies undes. \n",
            "\n",
            "'Wen yices had nows and she lookeds but sook have cas it as hand but not \n",
            "could at looked to the not c \n",
            "\n",
            " ra\n",
            " ra\n",
            " rauck, 'Well of the Mowhed gasted in that so kne in to We he said his mean. \n",
            "\n",
            "They \n",
            "the vissent revie, \n",
            "butt gouldn the \n",
            "breast old to \n",
            "the \n",
            "shabe bown have not loke of bances they \n",
            "\n",
            "Colch the reatred p \n",
            "\n",
            " he\n",
            " he\n",
            " heed by look had you \n",
            "singres all grees to gains and the mook-beyo, to done they reck aways of so of preacs \n",
            "in the great so and switten the \n",
            "\n",
            "he nor the Right bearous in you that hard. \n",
            "\n",
            "The not held,  \n",
            "\n",
            " lo\n",
            " lo\n",
            " loant in to this sives strings and have of the mast is the could wilk and Bight-now to the will of the looked of the gooked shouch them to may the fell beart ent lan! And pouring and \n",
            "\n",
            "they ssore the ra \n",
            "\n",
            " G\n",
            " G\n",
            " Ghave arlong \n",
            "the have ald not and cound gress and to the litt of the entil the Lave bout and now shots I defry. \n",
            "Sarummint and of the lookes, speat then \n",
            "but suppove the hornsing \n",
            "and at has fal. \n",
            "\n",
            "Su \n",
            "\n",
            " ra\n",
            " ra\n",
            " raomblast our to his there you gait the emoble. The Sleasingst darrs tomards \n",
            "gax same Gounting the musten \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "He kut is the Ridjuin. 'Gord, he said the \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Sounts you diving spack lo \n",
            "\n"
          ]
        }
      ],
      "source": [
        "for i in range(10):\n",
        "  start_strings = [\" Th\", \" wh\", \" he\", \" I \", \" ca\", \" G\", \" lo\", \" ra\"]\n",
        "  start = random.randint(0,len(start_strings)-1)\n",
        "  print(start_strings[start])\n",
        "#   all_characters.index(string[c])\n",
        "  print(evaluate(start_strings[start], 200), '\\n')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "n_epochs = 1000\n",
        "print_every = 200\n",
        "plot_every = 10\n",
        "hidden_size = 200\n",
        "n_layers = 3\n",
        "lr = 0.001\n",
        "\n",
        "decoder = Decoder_RNN(n_characters, hidden_size, n_characters, n_layers)\n",
        "decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=lr)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "start = time.time()\n",
        "all_losses = []\n",
        "loss_avg = 0"
      ],
      "metadata": {
        "id": "tlFl7SkMeIuh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# n_epochs = 2000\n",
        "\n",
        "file = unidecode.unidecode(open('./text_files/alma.txt').read())\n",
        "file_len = len(file)\n",
        "print('file_len =', file_len)\n",
        "\n",
        "for epoch in range(1, n_epochs + 1):\n",
        "  loss_ = train(*random_training_set())\n",
        "  loss_avg += loss_\n",
        "\n",
        "  if epoch % print_every == 0:\n",
        "      print('[%s (%d %d%%) %.4f]' % (time.time() - start, epoch, epoch / n_epochs * 100, loss_))\n",
        "      print(evaluate('Wh', 100), '\\n')\n",
        "\n",
        "  if epoch % plot_every == 0:\n",
        "      all_losses.append(loss_avg / plot_every)\n",
        "      loss_avg = 0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "egc48d3PeNLu",
        "outputId": "bfd0fa10-ee4d-42f0-8707-e3b57c5f7179"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "file_len = 466656\n",
            "[82.60434460639954 (200 20%) 1.9768]\n",
            "Wh\n",
            "Wh now Lacreperor wain ngthe wame dould se dorss and amand the sord, as the dey dey thes the min thaov \n",
            "\n",
            "[164.73279690742493 (400 40%) 1.7111]\n",
            "Wh\n",
            "Whings and they the sall tho kin to to the paest oth of that the dod, to ka the lard touk would in did \n",
            "\n",
            "[246.23968195915222 (600 60%) 1.4564]\n",
            "Wh\n",
            "Whren of the againt the word the Leren to pered the puses; grous that thou becould not men a mans, and \n",
            "\n",
            "[321.7609705924988 (800 80%) 1.7018]\n",
            "Wh\n",
            "Whed say bu words arnig: Amalies of the ware ent all but be inty ow the pely, as to bexce that in to t \n",
            "\n",
            "[396.7105941772461 (1000 100%) 1.7716]\n",
            "Wh\n",
            "Whing these ainsea, the year, and theer of there this were have to were this spopes, and desire lading \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "After running the text from scripture, it did pretty well.  Learning slowed around a loss of 1.6 and it didn't get much better than that.  This is after running it several times that I observed this.  It picks up key words from alma like Lamanites(not in this iteration), and basic sentence structure, but it struggles to make an actual passage that looks convincing or recognizable as from Alma.  Pretty cool over all."
      ],
      "metadata": {
        "id": "gjY2ZDdu0KPe"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "pycharm": {
      "stem_cell": {
        "cell_type": "raw",
        "metadata": {
          "collapsed": false
        },
        "source": []
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}